\documentclass[a4paper,10pt]{article}

\input{setup/preamble}
% \usepackage[a4paper, total={6.25in, 8in}]{geometry}
\input{setup/acronyms}

\title{Summary}
% \date{\today}
\date{}
% \author{Daniel Runge Petersen \and Lars Emanuel Hansen \and Sebastian Aaholm}
\pagestyle{empty}
\begin{document}
\maketitle
\thispagestyle{empty}
\noindent
This thesis introduces a symbolic approach to the \gls{bw} algorithm using \glspl{add} for efficiently learning parameters of \glspl{hmm} and \glspl{mc}.
Traditional recursive or matrix-based implementations of \gls{bw} often suffer from high memory usage and limited scalability.
To address this, we propose a novel symbolic implementation built into a C++ library named \Cupaal, which is integrated into the Python \Jajapy\ library as \JajapyTwo.


Tools like \Jajapy\ implement \gls{bw} recursively or through matrix operations.
However, these approaches become inefficient for large or repetitive models due to memory constraints and computational redundancy.

The motivation behind \Cupaal\ is to overcome these issues by leveraging \glspl{add}, data structures that compactly represent numerical functions over discrete variables.
\glspl{add} generalize \glspl{bdd}, allowing them to represent and manipulate probabilities and other numeric values symbolically.
This symbolic computation reduces redundancy and memory use, making it feasible to train large-scale probabilistic models.

\Cupaal\ represents transition, emission, and initial state matrices as \glspl{add}.
Matrix operations, including the Kronecker and Hadamard products, are symbolically implemented.
This structure is especially beneficial when matrix sparsity or redundancy exists.
PRISM models are translated into \Jajapy\ models and then encoded as \glspl{add} for use in \Cupaal.
The main contributions of this work are:


\begin{enumerate}
    \item Symbolic Reformulation of the \gls{bw} Algorithm: Each step of the \gls{bw} algorithm (forward-backward, parameter updates) is redefined in terms of \gls{add} operations using the \gls{cudd} library.
    \item Multi-Sequence Learning Support: The symbolic implementation handles multiple observation sequences for both \glspl{mc} and \glspl{hmm}.
    \item Integration with \Jajapy: The symbolic implementation is integrated into the \Jajapy library, allowing users to switch between traditional and symbolic learning modes.
    \item Empirical Evaluation: Using models from the QComp benchmark (specifically, the leader sync model), experiments demonstrate: Comparable or better runtime performance, especially for models with repeated structural patterns; Accuracy on par with traditional methods; Improved scalability, especially when models are initialized with repeated values (controlled initialization).
\end{enumerate}


The symbolic implementation of the \gls{bw} algorithm using \glspl{add} proves to be a scalable and efficient alternative to traditional approaches.
With \JajapyTwo, users now have access to a flexible tool for probabilistic model learning that maintains accuracy while improving performance for complex and large-scale models.
\end{document}