\section{Discussion}\label{sec:discussion}
This section discusses the results presented in Section~\ref{sec:results} and reflects on the performance of \Cupaal\ compared to \Jajapy.

The results from \autoref{sec:experiments} display that \Cupaal\ outperforms \Jajapy\ in terms of run time, especially for longer observations.
For models with few states the run time is similar, and only as the number of states increases the differences become more pronounced.

The experiment is conducted using a single model, which may not provide a comprehensive view of the performance across different scenarios.
A better approach would have been to use multiple models with varying characteristics and compare their performance.
This would allow for a more general overview of the performance of \Cupaal\ compared to \Jajapy.

A broader variety of models would give a clear insight in \Cupaal\ and hereby display the strenghs and weaknesses of \Cupaal.

Other models that were looked at were the NAND, and BRP model.....\todo{add other models}
As with only \texttt{Leader\_Sync} it is only clear that \Cupaal\ performs well with a large number of observations.

The model included could also have made use of a greater number of states and observations, currently the model only contains $\sim$2000 states.
There are models that contain a far greater amount of states, the impact of this could have been further explored.

In \todo{Ref to experiment 3} the values for the initial model values are not completly random, instead they are designed to have repeated values.
This was done to display the known strenghs of \Cupaal.

This does scew the model to favor \Cupaal\ as the \gls{add} structure benefits from repeated values and therefore will display results that indicate \Cupaal\ as the stronger implementation.

This was purely done to research what was believed to a strength of \Cupaal\ and to further the discussion of when \Cupaal\ is a good option to use over other tools such as \Jajapy.

% Intro to discussion
% Discussion of the results in section 6.
% We only use a single model, we should have used multiple models, and then compared them.
% Mini experiment three, some values are made less random, and instead have repeated values. - Is this a problem?
% Size of models

\subsection{Implementation Discussion} \label{subsec:implementation_discussion}
\Cupaal\ displays clear benefits when working with repeated values, but in general it struggels to compete with \Jajapy.
Previous work indicated that \Cupaal\ overall was a stronger implementation, but with a fully symbolic implementation some potential bottlenecks have been discovered.

Specifically in the update step of the \gls{bw} algorithm, as when working with \glspl{add} for just the Alpha and Beta steps, \Cupaal\ performed very well.

This indicates that there could be some issues when updating the values, when using \glspl{add}.
To further research this topic, a hybrid implementation could be provided.
This implementation would make use of \glspl{add} when calculating Alpha and Beta, and then use a recursive approach when updating values.
An implementation like this, would require a lot of convertion between matrices and \glspl{add}, but by comparing a fully symbolic, a recursive, and a hybrid approach would give further insight to what \Cupaal\ struggels with.

For now \Cupaal\ only measures the time taken for computing the \gls{bw} algorithm, but an interesting metric to compare would be the memory used.
If \Cupaal\ was discovered to require less memory than \Jajapy, even with more time needed for larger models, it could be a better choice.
But without any memory metric to compare, the decision can only be made based on the time needed for computing \gls{bw}.

To allow \Cupaal\ and \Jajapy\ to work together, Pybind11 was used to create bindings between the two.
This decision was done without exploring other options, therefore it might not be the best suited tool.
For now it was decided that bindings that worked was fine for the current iteration of \Cupaal, but further considerations should be put into whether or not a better tool exists.

The library used to manipulate \glspl{add}, was \gls{cudd} as it was what previous work had built upon.
A discussion back then was also whether or not this is the best tool for the job, as other tools such as Sylvan exist.
This discussion is still relevant, and worth exploring.

\Cupaal\ is designed for the \gls{bw} algorithm, but it would looking into other algorithms that could potentially benefit from a symbolic implementation.
An algorith that could be explored would be the \todo{add an algorithm here} algorithm.
By exploring other algorithms, general benefits of using a symbolic approach could be covered.

% Is there a bottleneck, such as the update step? - We saw that alpha and beta were not a bottleneck, but the update step might be.
% Would a hybrid approach be better, where we use a recursive implementation for the update step?

% We used Pybind11 to interface with C++, but we could have used other libraries.
% We could have used a different library for the symbolic engine, such as Sylvan or CUDD.
% Jajapy is shit, and we should not use it as a benchmark.
% We have only worked with the Baum-Welch algorithm, would ADDs also prove beneficial in other algorithms such as ****
%We dont measure memory, is this an issue and how could the results influence our conclusion of CuPAAL?

\subsection{Future Work Discussion}
Furthermore \Cupaal\ only makes use of a single core, this is not an issue when comparing it to \Jajapy\ as it can be limited to also only use a single core.
But an improvement to allow for multiple cores for \Cupaal\ could be a direction worth exploring.

% Would we be able to use ADDs for the observations?
% vi burde ogs√• lave en udgave af update steppet lavet til MCer, vi skal jo ikke opdatere omega der
% Cupaal only uses one core, so it is not parallelized.

% Did we achieve our goal of creating a symbolic implementation of the Baum-Welch algorithm?

% -----------------------------------------

% The accuracy of the models learned with the BW algorithm strongly depends on selecting an appropriate size for the output model. However, increasing this size substantially raises the computational cost of each update iteration, both in terms of time and space complexity.

% This is because each iteration requires running the forward-backward algorithm on every trace in the training set. In the original implementation, this step was performed using Jajapy models, incurring a cost of $O(n^2 \cdot K)$ in time and $O(n \cdot K)$ in space per iteration, where $n$ is the number of states in the output model and $K$ is the total number of label occurrences in the training set. Moreover, computing the updated transition probabilities from the forward and backward coefficients added an extra $O(n^2 \cdot K)$ overhead in both time and space.

% Unsurprisingly, this had a significant impact on the performance of the BW algorithm as the number of states increased.

% To address this limitation, CuPAAL\ introduces a symbolic engine that efficiently handles both the forward-backward computation and the parameter updates.


% As discussed in (Ref to previous section talking about Jajapy), Jajapy uses a recursive implementation of the Baum-Welch algorithm to learn \glspl{hmm}.
% In contrast, CuPAAL implements the Baum-Welch algorithm using \glspl{add}.
% By leveraging \glspl{add}, CuPAAL demonstrates significant improvements over the recursive approach used in Jajapy.
% The discussion of these improvements is based on the experimental results presented in Section~\ref{sec:experiments}.