\section{Preliminaries}\label{sec:preliminaries}
This section provides an overview of the theoretical background necessary to understand the rest of the article.
For ease of reference Appendix~\ref{sec:cheatsheet} contains a table of symbols used in the paper.

We begin by defining the key concepts of a \gls{hmm} and a \gls{mc}, which are the two main models used in this report, then go on to introduce the \gls{bw}, which is a widely used algorithm for training \glspl{hmm}, and showing how it can be adapted to handle multiple observation sequences using matrix operations.

\subsection{Hidden Markov Model}\label{subsec:hmm}
\acrfullpl{hmm} were introduced by Baum and Petrie in 1966~\cite{baum1966statistical} and have since been widely used in various fields, such as speech recognition~\cite{chavan2013overview}, bioinformatics~\cite{ciocchetta2009bio}, and finance~\cite{mamon2007hidden}.

A \gls{hmm} is a statistical model that describes a system that evolves over time.
The system is assumed to hold the Markov property, meaning that the future state of the system only depends on the current state and not on the past states.
The system is also assumed to be unobservable, meaning that the states are hidden and cannot be directly observed.
Instead, the system emits observations, which are used to infer the hidden states.


\begin{definition}[Hidden Markov Model]
    A Hidden Markov Model (HMM) is a tuple $\mathcal{M} = (S, L, \omega, \tau, \pi)$, where:
    \begin{itemize}
        \item $S$ is a finite set of states.
        \item $L$ is a finite set of labels.
        \item $\omega: S \rightarrow D(L)$ is the emission function.
        \item $\tau: S \rightarrow D(S)$ is the transition function.
        \item $\pi \in D(S)$ is the initial distribution.
    \end{itemize}
\end{definition}


$D(X)$ denotes the set of probability distributions over a finite set $X$.
The emission function $\omega$ describes the probability of emitting a label given a state.
The transition function $\tau$ describes the probability of transitioning from one state to another.
The initial distribution $\pi$ describes the probability of starting in a given state.

An example of a \gls{hmm} is a weather model where the hidden state represents the actual weather (sunny, rainy, or cloudy), but we only observe indirect signals, such as whether someone is carrying an umbrella or wearing sunglasses.

\subsection{Markov Chain}\label{subsec:mc}
A \acrfull{mc}, named after Andrei Markov, is a stochastic model widely used in different fields of study~\cite{Rabiner89}.


\begin{definition}[Markov Chain]
    A Markov Chain (MC) is a tuple $\mathcal{M} = (S, L, \omega, \tau, \pi)$ identical to the \gls{hmm} structure above except that the emission function is \emph{deterministic}: for every $s\in S$ there is a single label
    $l=\omega(s)$ emitted with probability 1.
\end{definition}


In other words, the emission function $\omega$ is a function that maps each state to a single label $l \in L$, meaning that each state emits exactly and only one label.
Two distinct states may emit the same label.

An example of a \gls{mc} is a board game where a player moves between squares based on dice rolls.
Each square corresponds to a state, the dice rolls determine the transition probabilities.

\subsection{Conversion between MCs and HMMs}\label{subsec:mc_hmm_conversion}
In this section, we will discuss the conversion between \glspl{mc} and \glspl{hmm}.
This conversion is important because it allows us to use the same algorithms and techniques from the original \Cupaal\ implementation for both model types, even though they have different properties.

In our case, we're interested in trace-equivalent models.
By trace-equivalent, we mean that the probability distribution over observed sequences is the same for both models.
i.e.\ the labels emitted by moving through the probabilistic models follow the same distribution.

From the definition of a \gls{mc}, we can see that it is a special case of an \gls{hmm} where the emission function is deterministic, which makes this conversion very simple.


\begin{definition}[Markov Chain to Hidden Markov Model]
    For each \gls{mc} $\mathcal{M} = (S, L, \omega, \tau, \pi)$, there exists a trace-equivalent \gls{hmm} $\mathcal{M}^{\prime} = (S^{\prime}, L^{\prime}, \omega^{\prime}, \tau^{\prime}, \pi^{\prime})$, where:
    \begin{itemize}
        \item $S^{\prime} = S$.
        \item $L^{\prime} = L$.
        \item $\omega^{\prime}(s)(l) = \begin{cases}
                      1 & l=\omega(s)      \\
                      0 & \text{otherwise}
                  \end{cases}$
        \item $\tau^{\prime} = \tau$.
        \item $\pi^{\prime} = \pi$.
    \end{itemize}
\end{definition}


The only difference in this case is the structure of the emission functions, thus preserving the probabilistic trace equivalence, i.e., for an arbitrary trace $O \in L^*$ we have $P[O \mid \mathcal{M}] = P[O \mid \mathcal{M}]$.

% \subsubsection{Hidden Markov Models to Markov Chains}\label{subsec:hmm2mc}
% Converting a \gls{hmm} to an equivalent \gls{mc} is more complex.
% In a \gls{hmm}, the observations are probabilistically related to the states, which introduces ambiguity, as multiple states can emit the same observation.
% To create a fully observable \gls{mc} that captures the behavior of a \gls{hmm}, we must encode both the hidden state and the emitted label into the state space.
% \begin{definition}[Hidden Markov Model to Markov Chain]
%     Conversely, let
%     \(
%     \mathcal{M}
%     = (S, L, \omega, \tau, \pi)
%     \)
%     be a \gls{hmm}.
%     We define the observable \gls{mc}
%     \(
%     \mathcal{M}^{\prime}
%     = (S^{\prime}, L^{\prime}, \omega^{\prime}, \tau^{\prime}, \pi^{\prime})
%     \)
%     by:
%     \begin{itemize}
%         \item $S^{\prime} = \{(s,l)\in S\timesL\}$%, the new state space encodes both hidden states and their possible emitted labels,
%         \item $L^{\prime} = L$%, the label space remains unchanged,
%         \item $\omega^{\prime}(s,l) = l$%, each new state deterministically emits its label,
%         \item $\tau^{\prime}\big((s,l),(s^{\prime},l^{\prime})\big) = \tau(s,s^{\prime})\;\omega(s^{\prime})(l^{\prime})$%, the transition function is modified to account for the emission probabilities,
%         \item $\pi^{\prime}(s,l) = \pi(s)\;\omega(s)(l)$%, the initial distribution is modified to account for the emission probabilities.
%     \end{itemize}
%     \textbf{Remark.}
%     Here each ^{\prime} symbol refers to an object of the \emph{derived MC}.  In
%     particular, $\tau^{\prime}$ is \emph{not} the original \gls{hmm} transition; it acts on the
%     expanded state space $S^{\prime}$ and already incorporates the emission probability for
%     the label $l^{\prime}$.

%     The mapping increases the state space size from $|S|$ to at most
%     $|S|\cdot|L|$, but yields a fully observable system amenable to
%     standard \gls{mc} analysis.
% \end{definition}

% The conversion between \glspl{mc} and \glspl{hmm} is important because it allows us to use the same algorithms and techniques for both models, even though they have different properties.

\subsection{Baum-Welch Algorithm}\label{subsec:baum-welch}
The \gls{bw} is a special case of the Expectation-Maximization (EM) framework used to estimate the parameters of a \gls{hmm} given a set of observed sequences.

Since the underlying states are not directly observable, the algorithm iteratively refines the model parameters $\pi$, $\omega$, and $\tau$ to maximize the likelihood of the observations.
Each iteration of the algorithm consists of two steps:


\begin{description}
    \item[E-step] Compute the expected values of the hidden variable given the current parameters.
    \item[M-step] Update the model parameters to maximize the expected complete-data log-likelihood.
\end{description}


Convergence is typically achieved when the change in the likelihood (or parameters) between iterations falls below a threshold~\cite{Rabiner89}.

We can represent the parameters of a \gls{hmm} as matrices for computational efficiency.

They are defined as follows:


\begin{description}
    \item[$\pmb{\pi}$] is the initial state distribution vector, where $\pi_i = \pi(s_i)$ is the probability of starting in state $s_i$, this is a column vector of size $|S|$.
    \item[$\pmb{\tau}$] is the transition matrix, where $\tau_{ij} = \tau(s_i)(s_j)$ is the probability of transitioning from state $s_i$ to state $s_j$, this is a square matrix of size $|S| \times |S|$.
    \item[$\pmb{\omega}$] is the emission matrix, where $\omega_{ij} = \omega(s_i)(l_j)$ is the probability of emitting label $l_j$ given state $s_i$, this is a matrix of size $|S| \times |L|$.
\end{description}


To illustrate our symbolic implementation, we describe a single Baum-Welch iteration in terms of matrix operations, assuming familiarity with the algorithm. For an introductory treatment, see~\cite{Baum70,reynouard2024learning}.

Let $\mathcal{M}$ denote the current \gls{hmm} hypothesis and let $O = o_1 \dots o_T$ be a sequence of observations, where each $o_t \in L$ and the observation sequence has the length $T$. Suppose $\mathcal{M}$ has $n$ states and $m$ labels, i.e., $S = {s_1, \dots, s_n}$, with parameters represented as follows:


\begin{itemize}
    \item $\pmb{\pi} \in [0,1]^{n}$ is the initial state distribution column vector.
    \item $\pmb{\tau} \in [0,1]^{n \times n}$ is the transition probability matrix.
    \item $\pmb{\omega} \in [0,1]^{n \times m}$ is the emission probability matrix.
\end{itemize}


The forward and backward algorithms are implemented using dynamic programming, as shown in \autoref{lst:forward-backward}.
For a given time step $t$, let $\pmb{\omega}(t)$ be the column vector of emission probabilities for label $o_t$ for each state, and $\odot$ the Hadamard (element-wise) product.


\begin{listing}[htb!]
    \begin{codebox}
        \Procname{\proc{Forward-Algorithm}}
        \li $\pmb{\alpha}(1) = \pmb{\omega}(1) \odot \pmb{\pi}$
        \li \For $t = 2$ \To $T$ \Do
        \li $\pmb{\alpha}(t) = \pmb{\omega}(t) \odot \left( \pmb{\tau}^\top \pmb{\alpha}(t-1) \right)$
        \End
    \end{codebox}
    \begin{codebox}
        \Procname{\proc{Backward-Algorithm}}
        \li $\pmb{\beta}(T) = \mathbf{1}$
        \li \For $t = T-1$ \To $1$ \Do
        \li $\pmb{\beta}(t) = \pmb{\tau} \left( \pmb{\beta}(t+1) \odot \pmb{\omega}(t+1) \right)$
        \End
    \end{codebox}
    \caption{Computation of the forward and backward coefficients}
    \label{lst:forward-backward}
\end{listing}


The above procedures compute the column vectors $\pmb{\alpha}(t)$ and $\pmb{\beta}(t) \in [0,1]^{n}$ for $t = 1\dots T$ which are later used to compute the coefficients $\pmb{\gamma}(t) \in [0,1]^{n}$ and $\pmb{\xi}(t) \in [0,1]^{n \times n}$ as follows:

\begin{align}
    \pmb{\gamma}(t) & = \pmb{\tau}[O | \mathcal{M}]^{-1} \cdot \pmb{\alpha}(t) \odot \pmb{\beta}(t)                                                                                 \\
    \pmb{\xi}(t)    & = (\pmb{\tau}[O | \mathcal{M}]^{-1} \cdot \pmb{\tau}) \odot \left( \pmb{\alpha}(t) \otimes \left(\pmb{\beta}(t+1) \odot \pmb{\omega}(t+1)\right)^\top \right)
\end{align}


Here, $\otimes$ is the Kronecker product and the probability $\pmb{\tau}[O | \mathcal{M}]$ to observe $O$ in $\mathcal{M}$ is computed as $\mathbf{1}^\top \pmb{\alpha}(T)$.
We calculate $\pmb{\gamma}(t)$ from $t= 1$ to $T$ and $\pmb{\xi}(t)$ from $t= 1$ to $T-1$.

Finally, the initial probability vector, the transition probability matrix and emission matrix and are updated as follows:


\begin{align}
    \hat{\pmb{\pi}}    & = \pmb{\gamma}(1)
    \label{eq:initial-probabilities-update}                                                                                          \\
    \hat{\pmb{\omega}} & = ( \mathbf{1} \oslash \pmb{\gamma}) \bullet \left(\sum_{t=1}^{T} \pmb{\gamma}(t) \otimes {[[o_t]]} \right)
    \label{eq:emission-probabilities-update}                                                                                         \\
    \hat{\pmb{\tau}}   & = (\mathbf{1} \oslash \pmb{\gamma} ) \bullet \pmb{\xi}
    \label{eq:transition-probabilities-update}
\end{align}


% \begin{equation}
%     \hat{\pmb{\pi}} = \pmb{\gamma}(1)
%     \label{eq:initial-probabilities-update}
% \end{equation}

% \begin{equation}
%     \hat{\pmb{\omega}} = ( \mathbf{1} \oslash \pmb{\gamma}) \bullet \left(\sum_{t=1}^{T} \pmb{\gamma}(t) \otimes {[[o_t]]} \right)
%     \label{eq:emission-probabilities-update}
% \end{equation}

% \begin{equation}
%     \hat{\pmb{\tau}} = (\mathbf{1} \oslash \pmb{\gamma} ) \bullet \pmb{\xi}
%     \label{eq:transition-probabilities-update}
% \end{equation}


Where $\bullet$ is the transposed Khatri-Rao product (i.e., row-by-row Kronecker product), and $[[o_t]] = ([[o_t=l]])_{l \in L}$ is the one-hot encoding of the observation $o_t$, meaning that it is a row vector of size $|L|$ with a 1 in the position corresponding to the observation $o_t$ and 0 elsewhere.
$\oslash$ is the element-wise division, and $\otimes$ is the Kronecker product.
The $\pmb{\gamma}$ and $\pmb{\xi}$ are defined as follows:


\begin{align}
    \pmb{\gamma} & = \sum_{t=1}^{T} \pmb{\gamma}(t) \\
    \pmb{\xi}    & = \sum_{t=1}^{T-1} \pmb{\xi}(t)
    \label{eq:gamma-xi-definitions}
\end{align}


These update rules form the standard \gls{bw} for training \glspl{hmm} on a single observation sequence.
However, the approach can be naturally extended to multiple sequences.


\subsection{Multiple Observation Sequences}\label{subsec:multiple-observation-sequences}
Suppose we are given a multiset of \gls{iid} observation sequences $\mathcal{O} = {O_1, O_2, \ldots, O_{|\mathcal{O}|}}$, where each $O_i = (o_{i1}, o_{i2}, \ldots, o_{iT})$ is of length $T$.
The E-step remains unchanged: for each sequence, we compute the corresponding $\pmb{\alpha}_i(t)$, $\pmb{\beta}_i(t)$, $\pmb{\gamma}_i(t)$, and $\pmb{\xi}_i(t)$ values independently.

In the M-step, we aggregate statistics across all sequences to update the parameters.
Specifically, we define:

\begin{align}
    \pmb{\gamma} & = \sum_{i=1}^{|\mathcal{O}|}\sum_{t=1}^{T} \pmb{\gamma}_{i}(t) \\
    \pmb{\xi}    & = \sum_{i=1}^{|\mathcal{O}|}\sum_{t=1}^{T-1} \pmb{\xi}_{i}(t)
\end{align}

With these aggregate quantities, the update rules for the initial distribution (see \autoref{eq:initial-probabilities-update}) and transition matrix (see \autoref{eq:transition-probabilities-update}) remain unchanged, because they are already defined in terms of the sum over all sequences.
However, the emission matrix update needs to account for all sequences.

The emission matrix is updated as follows:


\begin{equation}
    \pmb{\omega}
    _s(o) = (\mathbf{1} \oslash \pmb{\gamma}) \bullet \left( \sum_{i=1}^N \sum_{t=1}^{T} \pmb{\gamma}_i(t) \otimes [[o_{it}]] \right)
    \label{eq:omega-update}
\end{equation}


This mirrors the single-sequence case (see \autoref{eq:emission-probabilities-update}) but extends the summation in the left side of the Kronecker product to cover all sequences and all time steps.
This allows us to compute the emission probabilities for each state across all sequences, ensuring that the model captures the overall distribution of observations.

\subsection{Baum-Welch Algorithm for Markov Chains}\label{subsec:baum-welch-mc}
Since \glspl{mc} can be seen as \glspl{hmm} with deterministic emissions, where each state emits a unique observation with probability 1, the \gls{bw} simplifies accordingly when applied to \glspl{mc}.

In this case:

\begin{itemize}
    \item The forward and backward algorithms are computed identically to the \gls{hmm} case, but without weighting by emission probabilities, as these are implicitly handled by the observation sequence.
    \item The \textbf{E-step} computations for $\pmb{\gamma}(t)$ and $\pmb{\xi}(t)$ remain structurally the same, though emission terms are omitted due to determinism.
    \item The \textbf{M-step} updates for the initial distribution $\pmb{\pi}$ and the transition matrix $\pmb{\tau}$ are unchanged.
    \item The emission matrix $\pmb{\omega}$ is not updated, as it is fixed by the model's structure and need not be learned.
\end{itemize}

This simplification avoids unnecessary computation and reflects the reduced uncertainty in the model: there is no need to marginalize over emissions, as each state deterministically produces a known label.
Consequently, the \gls{bw} becomes more efficient when applied to \glspl{mc}.

\subsection{Decision Diagrams}\label{subsec:decision-diagrams}
\acrfullpl{bdd} are data structures for efficiently representing and manipulating Boolean functions.
They are a compressed representation of truth tables, capturing the logical structure of a function in a graph-based format by eliminating redundancy, reducing memory usage, and improving computational efficiency~\cite{bryant1986graph}.

A \gls{bdd} is a directed acyclic graph derived from a decision tree, where each non-terminal node represents a Boolean variable, edges correspond to binary assignments (0 or 1), and terminal nodes store function values (0 or 1).
To reduce the size of the decision tree, \glspl{bdd} exploit redundancy by merging equivalent substructures, resulting in a canonical form (when reduced and ordered) that allows for efficient operations such as function evaluation, equivalence checking, and Boolean operations~\cite{bryant1986graph}.

\glspl{bdd} have been widely used in formal verification, model checking, and logic synthesis due to their ability to compactly represent large Boolean functions while maintaining efficient computational properties.
However, in rare cases \glspl{bdd} can suffer from exponential blowup. This can occur particularly when dealing with functions that lack inherent structure or when representing numerical computations that go beyond Boolean logic.

\subsection{ADDs}\label{subsec:adds}
\acrfullpl{add} generalize the concept of \glspl{bdd} by allowing terminal nodes to take values beyond Boolean constants (0 and 1).
Instead of restricting values to true/false, \glspl{add} can store arbitrary numerical values, making them useful for representing and manipulating functions over discrete domains~\cite{bahar1997algebric}.
This generalization enables the efficient representation of functions such as cost functions~\cite{kwiatkowska2004probabilistic}, probabilities~\cite{baier1997symbolic}, and other numerical relationships that arise in probabilistic reasoning.

The fundamental structure of an \gls{add} remains similar to a \gls{bdd}, where a decision tree is compacted by merging redundant substructures.
However, instead of performing Boolean operations, \glspl{add} allow for arithmetic operations such as addition and multiplication, making them well-suited for representing matrices~\cite{bahar1997algebric}.