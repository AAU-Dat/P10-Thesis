%! Author = Runge
%! Date = 29-12-2023

\section{Introduction}\label{sec:introduction}
\IEEEPARstart{T}{he} Baum-Welch algorithm is a widely used method for training Markov models in applications such as speech recognition, bioinformatics, and financial modeling~\cite{chavan2013overview,ciocchetta2009bio,mamon2007hidden}.

Traditionally, the Baum-Welch algorithm relies on matrix-based or recursive approaches to estimate model parameters from observed sequences.

An example of this is the \Jajapy\ library~\cite{ReynouardIB23}, which implements the Baum-Welch algorithm using a recursive matrix-based approach.
This library is designed to learn probabilistic models from partially observable executions, producing observation sequences - also known as traces.

The key strength of \Jajapy\ lies in its flexibility to accommodate various learning scenarios, along with seamless integration into standard verification workflows using tools like \Storm\ and \Prism.
However, the performance of \Jajapy\'s Baum-Welch algorithm implementation has been a significant limitation, because of the inherent redundancy in matrix-based representations, which leads to inefficiencies particularly in terms of time and memory consumption, which restricts its scalability to larger models.

To address these challenges, we propose a novel approach that replaces conventional matrices and recursive formulations with \glspl{add}.
\glspl{add} provide a compact, structured representation of numerical functions over discrete variables, enabling efficient manipulation of large probabilistic models.

By leveraging \glspl{add}, we can exploit the sparsity and structural regularities of \glspl{hmm} and \glspl{mc}, significantly reducing memory consumption and accelerating computation.

This paper presents several contributions toward efficient learning of \glspl{hmm} and \glspl{mc} models, by leveraging \glspl{add}:

First, we extend the \gls{bw} algorithm for these models using symbolic computation, reformulating each algorithm step as operations on \glspl{add}, leveraging the CUDD library to carry out these operations symbolically using \glspl{add}.
This reformulation enables efficient calculation of the Markov models in a compact and scalable form.

Secondly, our approach extends previous work on symbolic calculation by accommodating learning from multiple observation sequences for both types of Markov models, broadening the applicability of symbolic learning.

Thirdly, we conduct an experimental evaluation of the scalability of the symbolic Baum-Welch algorithm for a \gls{mc} from the QComp benchmark set~\cite{HartmannsKPQR19}, which serves as a standard reference for evaluating the performance of probabilistic model checking algorithms.
Our findings suggest that replacing matrices and recursive formulations with \glspl{add} offers a scalable alternative, making Markov model-based learning feasible for larger and more complex datasets.

Additionally, we implement Python bindings for the \Cupaal\ tool, making it accessible and usable within Python-based machine learning and model-checking workflows, such as \Jajapy\footnote{Source code avaliable at: \url{https://github.com/AAU-Dat/CuPAAL}}.

Finally, using theses python bindings we integrate \Cupaal\ into \Jajapy\ as \JajapyTwo, enabling users to seamlessly run symbolic probabilistic learning algorithms within \Jajapy.
