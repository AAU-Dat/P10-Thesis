%! Author = Runge
%! Date = 29-12-2023

\section{Introduction}\label{sec:introduction}
The Baum-Welch algorithm is a widely used method for training markov models in applications such as speech recognition, bioinformatics, and financial modeling~\cite{chavan2013overview,ciocchetta2009bio,mamon2007hidden}.

Traditionally, the Baum-Welch algorithm relies on matrix- or recursive-based approaches to estimate model parameters from observed sequences.

However, these methods can become computationally expensive, particularly for large state spaces and long observation sequences.
The inherent redundancy in matrix-based representations leads to inefficiencies in both time and memory usage, limiting scalability in practical applications.

To address these challenges, we propose a novel approach that replaces conventional matrices and recursive formulations with \glspl{add}.
\glspl{add} provide a compact, structured representation of numerical functions over discrete variables, enabling efficient manipulation of large probabilistic models.

By leveraging \glspl{add}, we can exploit the sparsity and structural regularities of HMMs, significantly reducing memory consumption and accelerating computation.

This paper explores the integration of glspl{add} into the Baum-Welch algorithm, demonstrating how this approach improves efficiency while maintaining numerical accuracy.

The integration of \glspl{add}, will be compared to the python library Jajapy, which implements a recursive approach of matrix representation.
These comparisons will include scalability, run-time, number of interations, log-likelihood etc.

Our findings suggest that replacing matrices and recursive formulations with \glspl{add} offers a scalable alternative, making Markov model-based learning feasible for larger and more complex datasets.