%! Author = Runge
%! Date = 29-12-2023

\section{Introduction}\label{sec:introduction}
The Baum-Welch algorithm is a widely used method for training markov models in applications such as speech recognition, bioinformatics, and financial modeling~\cite{chavan2013overview,ciocchetta2009bio,mamon2007hidden}.

Traditionally, the Baum-Welch algorithm relies on matrix-based or recursive approaches to estimate model parameters from observed sequences.

However, these methods can become computationally expensive, particularly for large state spaces and long observation sequences.
The inherent redundancy in matrix-based representations leads to inefficiencies in both time and memory usage, limiting scalability in practical applications.

To address these challenges, we propose a novel approach that replaces conventional matrices and recursive formulations with \glspl{add}.
\glspl{add} provide a compact, structured representation of numerical functions over discrete variables, enabling efficient manipulation of large probabilistic models.

By leveraging \glspl{add}, we can exploit the sparsity and structural regularities of HMMs, significantly reducing memory consumption and accelerating computation.

This paper explores the integration of \glspl{add} into the Baum-Welch algorithm, demonstrating how this approach enhances efficiency while preserving numerical accuracy.

The proposed method is implemented in the tool CuPAAL, which will be compared to the Python library Jajapy, an existing implementation that employs a recursive matrix-based approach. These comparisons will evaluate key factors such as scalability, runtime, number of iterations, and log-likelihood.

Our findings suggest that replacing matrices and recursive formulations with \glspl{add} offers a scalable alternative, making Markov model-based learning feasible for larger and more complex datasets.