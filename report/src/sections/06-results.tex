\begin{figure*}[htb!]
    \centering
    \input{figures/results/3d-scalability.pgf}
    \caption{Plot of the run time of \Jajapy\ and \Cupaal\ for the leader sync models, given the number of states and the length of the observations. The planes are linear regression fits to indicate the directions of the trends for the datapoints of similar color.}
    \label{fig:leader_results}
\end{figure*}

\section{Results}\label{sec:results}
In this section, we present the results of our experiments, which are divided into two main parts: the first part focuses on the scalability of \Jajapy\ and \Cupaal\ in terms of time and scalability, while the second part evaluates the accuracy of both tools.

The experiments were conducted on a machine with the specifications and environment listed in \autoref{sec:machine_specs}.


\subsection{Scalability}\label{subsec:scalability}
These results are the time taken to train a model, based on two parameters: the number of states, and the length of the observations in the model increasing.


\begin{table}[htb!]
    \centering
    \caption{Leader Sync model variations in training time in seconds.}
    \label{tab:leader_results}
    \begin{tabular}{lrrrr}
        \toprule
        model & states & length & jajapy (s) & cupaal (s) \\
        \midrule
        3.2   & 26     & 25     & 1.38       & 0.26       \\
        3.2   & 26     & 50     & 1.95       & 0.14       \\
        3.2   & 26     & 100    & 4.09       & 0.23       \\
        3.3   & 69     & 25     & 7.95       & 2.46       \\
        3.3   & 69     & 50     & 11.20      & 1.59       \\
        3.3   & 69     & 100    & 19.65      & 1.75       \\
        3.4   & 147    & 25     & 27.10      & 8.54       \\
        3.4   & 147    & 50     & 42.57      & 9.20       \\
        3.4   & 147    & 100    & 84.02      & 9.90       \\
        4.2   & 61     & 25     & 15.68      & 11.18      \\
        4.2   & 61     & 50     & 24.87      & 13.56      \\
        4.2   & 61     & 100    & 52.11      & 11.24      \\
        4.3   & 274    & 25     & 194.88     & 231.28     \\
        4.3   & 274    & 50     & 414.30     & 379.21     \\
        4.3   & 274    & 100    & 447.83     & 117.78     \\
        4.4   & 812    & 25     & 1846.68    & 3324.83    \\
        4.4   & 812    & 50     & 2290.28    & 1848.44    \\
        4.4   & 812    & 100    & 5652.14    & 3447.56    \\
        5.2   & 141    & 25     & 95.59      & 104.71     \\
        5.2   & 141    & 50     & 342.05     & 553.66     \\
        5.2   & 141    & 100    & 798.73     & 982.97     \\
        5.3   & 1050   & 25     & 4586.86    & 10906.91   \\
        5.3   & 1050   & 50     & 7791.95    & 10405.75   \\
        5.3   & 1050   & 100    & 9821.74    & 5992.51    \\
        \bottomrule
    \end{tabular}
\end{table}


The results for the leader sync model are displayed in \autoref{tab:leader_results} and \autoref{fig:leader_results}, and show the time it takes to train a model, given the number of states and observation length.
Only the training time is considered; the initialization of the programs is not a factor in these numbers.

In \autoref{fig:leader_results}, simple planes are fit with linear regression from the data in \autoref{tab:leader_results}.
This is not an attempt to say anything definitive about the degrees of the scaling, but instead to show the generally observable trend.

Contrary to our expectations, the data does not show a clear difference in the time taken to train the leader sync model between \Jajapy\ and \Cupaal\ for \glspl{dtmc}.

For very small models, the running time does not matter too much, but we observe an initial overhead related to \Jajapy. This is likely related to the general consensus that Python is a slower language than C in general.

Generally, more states mean longer running time, but interestingly, variations with similar number of states may have very different training times.
The most obvious example is the 3.4 and 5.2 models, with 147 and 141 states respectively.
The 5.2 model is much slower, especially in \Cupaal, showing a \textasciitilde10 times increase in training time, despite having slightly fewer states.

Initially, we only had data for observations of length 25, and the data under those conditions suggested that \Jajapy\ scaled quite a bit better than \Cupaal.

\begin{figure}[htb!]
    \input{figures/results/cupaal-length-to-runtime.pgf}
    \caption{\Cupaal\ runtimes with increasing observation length.}
    \label{fig:cupaal-length-to-runtime}
\end{figure}


\begin{figure}[htb!]
    \input{figures/results/jajapy-length-to-runtime.pgf}
    \caption{\Jajapy\ runtimes with increasing observation length.}
    \label{fig:jajapy-length-to-runtime}
\end{figure}


To explore this behaviour, we extended the experiment to contain data for observations of different lengths, and now our observations are more in line with our expectations.
\Jajapy gets slower at a pace roughly linear with the length of the observations; doubling the observation length doubles the run time of \Jajapy.
This is not the case for \Cupaal, where we do not see any particular increase in running time as the observation length increases.

In fact, looking at \autoref{fig:cupaal-length-to-runtime} and \autoref{fig:jajapy-length-to-runtime}, the \Cupaal\ runtimes look a little strange.




% This is likely due to the smaller size of the model, not having as much redundant calculations to avoid, which is the main advantage of the symbolic approach used in CuPAAL.
% Mention the repeat calculations of a model that has reached a steady-state, and therefore have infinitely repeated observations.


From \autoref{fig:leader_results}




\subsection{Accuracy}\label{subsec:accuracy}
This section will cover the second experiment done for comparing \Cupaal\ and \Jajapy.
This experiment explores the effect of random and semi-random initial model parameters, as we expect repeated values to be highly beneficial for \Cupaal's implementation.

Table \autoref{tab:leader_results_loglikelihood}, displays three models with a varrying number og states and observation sequences.
This table is used to highlight that the different methods of initialising model parameters, has an impact of the number of iterations needed, but not showing a clear strength in either method.
This is to be expected, as depending on how close or far off, the values are to the learned model, has an impact on the number of iterations needed.
But it also showcases that there is no impact on the loglikelihood, meaning that no matter the method used the model learned is still equally close to the correct model.


\begin{table}[htb!]
    \centering
    \caption{Leader Sync model variations in loglikelihood for random and semi-random initial values.}
    \label{tab:leader_results_loglikelihood}
    \begin{tabular}{rrrrrrrrrrr}
        \toprule
        $\mathcal{M}$ & $|S|$ & $|O|$ & iter(rand) & iter(semi) & $\ell$ rand & $\ell$ semi \\
        \midrule
        4.2           & 61    & 25    & 18         & 16         & -140.41     & -140.41     \\
        4.2           & 61    & 50    & 17         & 20         & -149.13     & -149.13     \\
        4.2           & 61    & 75    & 17         & 18         & -117.80     & -117.80     \\
        4.2           & 61    & 100   & 17         & 18         & -138.63     & -138.63     \\
        4.3           & 274   & 25    & 18         & 18         & -79.92      & -79.92      \\
        4.3           & 274   & 50    & 18         & 18         & -67.24      & -67.25      \\
        4.3           & 274   & 75    & 18         & 18         & -65.71      & -65.71      \\
        4.3           & 274   & 100   & 18         & 18         & -62.55      & -62.55      \\
        4.4           & 812   & 25    & 18         & 18         & -62.55      & -62.55      \\
        4.4           & 812   & 50    & 18         & 18         & -48.49      & -48.49      \\
        4.4           & 812   & 75    & 17         & 18         & -52.26      & -52.26      \\
        4.4           & 812   & 100   & 18         & 18         & -65.71      & -65.71      \\
        \bottomrule
    \end{tabular}
\end{table}

\autoref{tab:leader_results_rand_vs_semi} is the table that compares \Cupaal\ to \Jajapy, and the impact of random and semi-random initialization, in regards to the time needed to learn the model.





\begin{table}
    \centering
    \caption{Leader Sync model variations in training time with random and semi-random initial values.}
    \label{tab:leader_results_rand_vs_semi}
    \begin{tabular}{rrrrrrrrrrr}
        \toprule
        $\mathcal{M}$ & $|S|$ & $|O|$ & rand-ja & rand-cup & semi-ja & semi-cup \\
        \midrule
        4.2           & 61    & 25    & 15.68   & 11.12    & 13.26   & 7.87     \\
        4.2           & 61    & 50    & 24.87   & 13.56    & 28.32   & 12.54    \\
        4.2           & 61    & 75    & 36.02   & 9.72     & 38.62   & 10.54    \\
        4.2           & 61    & 100   & 52.11   & 11.24    & 53.75   & 10.95    \\
        4.3           & 274   & 25    & 194.88  & 231.28   & 190.65  & 194.04   \\
        4.3           & 274   & 50    & 414.30  & 379.21   & 414.95  & 308.81   \\
        4.3           & 274   & 75    & 476.68  & 232.21   & 486.04  & 206.67   \\
        4.3           & 274   & 100   & 447.83  & 117.78   & 441.40  & 118.17   \\
        4.4           & 812   & 25    & 1846.67 & 3324.83  & 1793.90 & 3087.66  \\
        4.4           & 812   & 50    & 2290.28 & 1848.44  & 2239.18 & 1526.81  \\
        4.4           & 812   & 75    & 3017.72 & 1597.78  & 3177.81 & 1512.33  \\
        4.4           & 812   & 100   & 5652.14 & 3447.56  & 5586.23 & 2959.75  \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}
    \centering
    \caption{Leader Sync model variations in accuracy for random and semi-random initial values.}
    \label{tab:leader_results_accuracy}
    \begin{tabular}{rrrrr}
        \toprule
        model & obscount & original & jajapy & cupaal \\
        \midrule
        3.2   & 25       & 1.33     & 1.28   & 1.28   \\
        3.2   & 75       & 1.33     & 1.30   & 1.30   \\
        3.2   & 100      & 1.33     & 1.28   & 1.28   \\
        3.3   & 25       & 1.12     & 1.23   & 1.23   \\
        3.3   & 50       & 1.12     & 1.11   & 1.11   \\
        3.3   & 100      & 1.12     & 1.13   & 1.13   \\
        3.4   & 25       & 1.07     & 1.08   & 1.08   \\
        3.4   & 50       & 1.07     & 1.09   & 1.09   \\
        3.4   & 100      & 1.07     & 1.11   & 1.11   \\
        4.2   & 25       & 2.00     & 2.11   & 2.12   \\
        4.2   & 50       & 2.00     & 2.16   & 2.16   \\
        4.2   & 100      & 2.00     & 2.00   & 2.00   \\
        4.3   & 25       & 1.35     & 1.37   & 1.37   \\
        4.3   & 50       & 1.35     & 1.28   & 1.28   \\
        4.3   & 100      & 1.35     & 1.25   & 1.28   \\
        4.4   & 25       & 1.19     & 1.25   & 1.25   \\
        4.4   & 50       & 1.19     & 1.17   & 1.17   \\
        4.4   & 100      & 1.19     & 1.27   & 1.27   \\
        5.2   & 25       & 3.20     & 3.27   & 3.27   \\
        5.2   & 50       & 3.20     & 3.06   & 3.06   \\
        5.2   & 100      & 3.20     & 3.50   & 3.50   \\
        5.3   & 25       & 1.35     & 1.32   & 1.32   \\
        5.3   & 50       & 1.35     & 1.27   & 1.27   \\
        5.3   & 100      & 1.35     & 1.33   & 1.33   \\
        \bottomrule
    \end{tabular}
\end{table}