\begin{figure*}
    \centering
    \input{figures/results/3d-scalability.pgf}
    \caption{Plot of the run time of \Jajapy\ and \Cupaal\ for the leader sync models, given the number of states and the length of the observations. The planes are linear regression fits to indicate the directions of the trends for the datapoints of similar color.}
    \label{fig:leader_results}
\end{figure*}


\section{Results}\label{sec:results}
In this section, we present the results of our experiments, which are divided into three main parts. The first part focuses on the scalability of \Jajapy\ and \Cupaal\ in terms of time and scalability.
The second part evaluates the accuracy of both tools, while the third part assesses the impact of using controlled initialization for the initial parameters.

The experiments were conducted on a machine with the specifications and environment listed in \autoref{sec:machine_specs}.

\subsection{Accuracy}\label{subsec:accuracy}
This experiment compares the accuracy of \Cupaal\ and \Jajapy\ in learning the leader sync model. Specifically, we model-check how many rounds it takes for each model to select a new leader, starting from the original model, \Jajapy\ and \Cupaal\, using properties from \autoref{lst:leader-sync-additions}.

\autoref{tab:leader-results-accuracy} shows the results. The table includes the number of rounds for the original model (column \textbf{rounds}) and the learned models from \Jajapy and \Cupaal.
The $\phi$ column shows the relative error between \Jajapy\ and the true value from the original model.
The results show that both \Cupaal\ and \Jajapy\  implementations learned the same model, and they both closely match the true model. For example, in the row for model 3.2 with 25 observations, the original model takes 1.33 rounds, and both \Cupaal\ and \Jajapy\  predict 1.28 rounds.
The relative error $\phi$ indicates a minimal deviation from the true value.

In conclusion, these results demonstrate that \Cupaal\ is a reliable and accurate implementation of the \gls{bw} algorithm, achieving performance comparable to that of \Jajapy\ in learning the model.

\begin{table}
    \centering
    \caption{Leader sync model variations in training time in seconds.}
    \label{tab:leader_results}
    \begin{tabular}{lrrrr}
        \toprule
        $\mathcal{M}$ & $|S|$ & $T$ & jajapy (s) & cupaal (s) \\
        \midrule
        3.2           & 26    & 25  & 1.38       & 0.26       \\
        3.2           & 26    & 50  & 1.95       & 0.14       \\
        3.2           & 26    & 100 & 4.09       & 0.23       \\
        3.3           & 69    & 25  & 7.95       & 2.46       \\
        3.3           & 69    & 50  & 11.20      & 1.59       \\
        3.3           & 69    & 100 & 19.65      & 1.75       \\
        3.4           & 147   & 25  & 27.10      & 8.54       \\
        3.4           & 147   & 50  & 42.57      & 9.20       \\
        3.4           & 147   & 100 & 84.02      & 9.90       \\
        4.2           & 61    & 25  & 15.68      & 11.18      \\
        4.2           & 61    & 50  & 24.87      & 13.56      \\
        4.2           & 61    & 100 & 52.11      & 11.24      \\
        4.3           & 274   & 25  & 194.88     & 231.28     \\
        4.3           & 274   & 50  & 414.30     & 379.21     \\
        4.3           & 274   & 100 & 447.83     & 117.78     \\
        4.4           & 812   & 25  & 1846.68    & 3324.83    \\
        4.4           & 812   & 50  & 2290.28    & 1848.44    \\
        4.4           & 812   & 100 & 5652.14    & 3447.56    \\
        5.2           & 141   & 25  & 95.59      & 104.71     \\
        5.2           & 141   & 50  & 342.05     & 553.66     \\
        5.2           & 141   & 100 & 798.73     & 982.97     \\
        5.3           & 1050  & 25  & 4586.86    & 10906.91   \\
        5.3           & 1050  & 50  & 7791.95    & 10405.75   \\
        5.3           & 1050  & 100 & 9821.74    & 5992.51    \\
        \bottomrule
    \end{tabular}
\end{table}


\begin{figure}
    \input{figures/results/cupaal-length-to-runtime.pgf}
    \caption{\Cupaal\ runtimes with increasing observation length.}
    \label{fig:cupaal-length-to-runtime}
\end{figure}


\begin{figure}
    \input{figures/results/jajapy-length-to-runtime.pgf}
    \caption{\Jajapy\ runtimes with increasing observation length.}
    \label{fig:jajapy-length-to-runtime}
\end{figure}


\subsection{Scalability}\label{subsec:scalability}
These results represent the time taken to train a model based on two parameters: the number of states and the length of the observations in the model, which increase.

The results for the leader sync model are presented in \autoref{tab:leader_results} and \autoref{fig:leader_results}, showing the time required to train a model based on the number of states and observation length. Only the training time is considered; the initialization of the programs is not a factor in these numbers.

In \autoref{fig:leader_results}, simple planes are fit with linear regression from the data in \autoref{tab:leader_results}.
This is not an attempt to make any definitive statements about the degrees of scaling but rather to illustrate the generally observable trend.
Contrary to our expectations, the data does not show a clear difference in the time taken to train the leader sync model between \Jajapy\ and \Cupaal\ for \glspl{dtmc}.

For very small models, the running time does not matter too much; however, we observe an initial overhead in \Jajapy\ compared to \Cupaal. This is likely related to the consensus that Python is generally slower than C.

More states mean longer running time, but interestingly, variations with a similar number of states may have very different training times.
The most obvious example is the 3.4 and 5.2 models, which have 147 and 141 states, respectively.
The 5.2 model is significantly slower, especially in \Cupaal, exhibiting a nearly 10 times increase in training time despite having slightly fewer states.

Initially, we only had data for observations of length 25, and the data under those conditions suggested that \Jajapy\ scaled significantly better than \Cupaal.

To explore this behavior, we extended the experiment to include data for observations of different lengths, and our observations are now more in line with our expectations.
\Jajapy\ gets slower at a pace roughly linear with the length of the observations; doubling the observation length doubles the run time of \Jajapy.
This is not the case for \Cupaal, where we do not see any particular increase in running time as the observation length increases.

Looking at \autoref{fig:cupaal-length-to-runtime} and \autoref{fig:jajapy-length-to-runtime}, the \Cupaal\ runtimes decrease as the observation length increases.
This is contrary to what is expected, as one would assume that with more data, the calculations either increase the running time or have no effect on it.
Our hypotheses as to why this is the case for \autoref{fig:cupaal-length-to-runtime} are due to the model this experiment is based on.

Since \texttt{Leader\_Sync} usually ends its observations with the label Elected.
As the observation length increases, more of this label appears in sequence.

This leads the \gls{bw} algorithm to learn the final part of the model more quickly and, at a certain point, reach a constant value that it can reuse in future computations, thereby accelerating the learning of part of the model.


\begin{table}
    \centering
    \caption{Leader sync learning tool model comparison of \Cupaal, \Jajapy, and the reference model. $R$ represents the reward model from~\autoref{lst:leader-sync-additions}. $\phi$ represents the difference between \Cupaal, \Jajapy, and the reference model.}
    \label{tab:leader-results-accuracy}
    \begin{tabular}{rrrrrrrr}
        \toprule
        $\mathcal{M}$ & $T$ & $R$  & $R_{ja}$ & $R_{cup}$ & $\phi$ & $\ell$ cup & $\ell$ ja \\
        \midrule
        3.2           & 25  & 1.33 & 1.28     & 1.28      & 0.04   & -98.72     & -98.72    \\
        3.2           & 75  & 1.33 & 1.30     & 1.30      & 0.02   & -70.23     & -70.23    \\
        3.2           & 100 & 1.33 & 1.28     & 1.28      & 0.04   & -67.24     & -67.24    \\
        3.3           & 25  & 1.12 & 1.23     & 1.23      & 0.09   & -59.27     & -59.27    \\
        3.3           & 50  & 1.12 & 1.11     & 1.11      & 0.01   & -35.87     & -35.87    \\
        3.3           & 100 & 1.12 & 1.13     & 1.13      & 0.00   & -40.33     & -40.33    \\
        3.4           & 25  & 1.07 & 1.08     & 1.08      & 0.01   & -28.52     & -28.52    \\
        3.4           & 50  & 1.07 & 1.09     & 1.09      & 0.02   & -31.07     & -31.07    \\
        3.4           & 100 & 1.07 & 1.11     & 1.11      & 0.04   & -35.87     & -35.87    \\
        4.2           & 25  & 2.00 & 2.11     & 2.12      & 0.06   & -140.41    & -140.41   \\
        4.2           & 50  & 2.00 & 2.16     & 2.16      & 0.08   & -149.13    & -149.13   \\
        4.2           & 100 & 2.00 & 2.00     & 2.00      & 0.00   & -138.63    & -138.63   \\
        4.3           & 25  & 1.35 & 1.37     & 1.37      & 0.01   & -79.92     & -79.92    \\
        4.3           & 50  & 1.35 & 1.28     & 1.28      & 0.05   & -67.24     & -67.24    \\
        4.3           & 100 & 1.35 & 1.25     & 1.25      & 0.07   & -62.55     & -62.55    \\
        4.4           & 25  & 1.19 & 1.25     & 1.25      & 0.05   & -62.55     & -62.55    \\
        4.4           & 50  & 1.19 & 1.17     & 1.17      & 0.01   & -48.50     & -48.50    \\
        4.4           & 100 & 1.19 & 1.27     & 1.27      & 0.07   & -65.71     & -65.71    \\
        5.2           & 25  & 3.20 & 3.27     & 3.27      & 0.02   & -155.11    & -155.11   \\
        5.2           & 50  & 3.20 & 3.06     & 3.06      & 0.04   & -185.72    & -185.72   \\
        5.2           & 100 & 3.20 & 3.50     & 3.50      & 0.09   & -208.39    & -208.39   \\
        5.3           & 25  & 1.35 & 1.32     & 1.32      & 0.02   & -73.11     & -73.11    \\
        5.3           & 50  & 1.35 & 1.27     & 1.27      & 0.06   & -65.71     & -65.71    \\
        5.3           & 100 & 1.35 & 1.33     & 1.33      & 0.01   & -74.52     & -74.52    \\
        \bottomrule
    \end{tabular}
\end{table}


\begin{figure}
    \input{figures/experiment3/model4.2semirandom-cupaal-jajapy-runtime.pgf}
    \caption{Model 4.2 - Runtime for random and controlled initialization}
    \label{fig:semirandom-cupaal-jajapy-4-2}
\end{figure}

\begin{figure}
    \input{figures/experiment3/model4.3semirandom-cupaal-jajapy-runtime.pgf}
    \caption{Model 4.3 - Runtime for random and controlled initializationm}
    \label{fig:semirandom-cupaal-jajapy-4-3}
\end{figure}

\begin{figure}
    \input{figures/experiment3/model4.4semirandom-cupaal-jajapy-runtime.pgf}
    \caption{Model 4.4 - Runtime for random and controlled initialization}
    \label{fig:semirandom-cupaal-jajapy-4-4}
\end{figure}

\subsection{Controlled Initialization}\label{subsec:controled_initialization}
This section will cover the third experiment, which compares \Cupaal\ and \Jajapy.
This experiment explores the effect of random and controlled initial model parameters, as we expect repeated values to be highly beneficial for \Cupaal’s implementation.

\autoref{tab:leader_results_rand_vs_semi} compares \Cupaal\ to \Jajapy and the impact of random and controlled initialization on the time needed to learn the model.

Examining the columns \texttt{rand-ja} and \texttt{control-ja}, which show the impact of random and controlled initialization for \Jajapy, reveals no significant difference between the two.

However, when examining the differences for \Cupaal\, it is clear that the controlled approach is generally faster than the completely random one; this becomes especially pronounced as the number of states in the model increases.
We expected this to be the case, as the number of observations resulted in more repeated values, and the controlled initialization had a similar effect.
This seems to partially alleviate the poor scaling \Cupaal\ suffers from as the number of model states increases.

\autoref{tab:leader_results_rand_vs_semi} presents three models with varying numbers of states and observation sequences. This table highlights that the different methods of initializing model parameters have an impact on the number of iterations required but do not demonstrate a clear advantage in either method.

This is to be expected, as the distance between the values and the learned model has an impact on the number of iterations needed.
Both the random and controlled initialization resulted in identical log-likelihoods, meaning that regardless of the method used, the learned model is still equally close to the correct model.

\Cref{fig:semirandom-cupaal-jajapy-4-2,fig:semirandom-cupaal-jajapy-4-3,fig:semirandom-cupaal-jajapy-4-4} give a visual representation of how \Cupaal\ compares to \Jajapy\ based on the observation counts while using both random and controlled initialization.

These graphs illustrate the general trend of \Cupaal\ performing slightly better with controlled initialization, whereas for \Jajapy, we observe no clear tendency for what performs best.

\autoref{tab:leader_results_rand_vs_semi} also shows the seconds per iteration to compute \Cupaal\ and \Jajapy\ for both random and controlled initialization.

\Jajapy\ is not noticeably affected by either method, yielding a gain of about $1\%$ when using the controlled initialization over the random approach; this can be attributed to coincidence, as the differences are negligible.
\Cupaal\ does have a slight gain when using the controlled approach, showcasing a gain of $\sim 11\%$ less time needed pr. Iteration compared to the completely random approach.

These results indicate a gain for \Cupaal\ when there are repeated values for the initialization. This lines up well with our hypothesis that \Cupaal\ performs better when it can leverage repeated values for its \gls{add} structure.