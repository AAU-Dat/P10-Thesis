\section{Discussion}\label{sec:discussion}
In this section, we discuss the results presented in \autoref{sec:results} and reflect on the performance of \Cupaal\ compared to \Jajapy.

The results from \autoref{sec:results} show that \Cupaal\ outperforms \Jajapy\ in terms of run time as the observation length increases.
For models with a small number of states, the runtime is similar.
However, as the number of states increases, the differences become more pronounced, which indicates that \Cupaal\ is a better choice for larger observation sequences. However, \Jajapy\ might still be a better choice for large models.

The experiment is conducted using a single model, Leader Sync, which may not provide a comprehensive view of performance across different scenarios.

A more effective approach would have been to utilize multiple models with varying characteristics and compare their performance, which would allow for a more comprehensive overview of \Cupaal's performance compared to \Jajapy.

Other models that were considered were the NAND and BRP models, also from the qcomp benchmark set~\cite{hartmanns2019quantitative}.
A wider variety of models would provide more and clearer insight into \Cupaal, thereby displaying its strengths and weaknesses.

The model could also have utilized a greater number of states and observations; currently, the largest model contains 1,050 states and observation sequences of length 10.

Larger models were considered, but it was determined that the largest model used was sufficient, given the time required for parameter estimation.

We ran the experiments in a Docker container, which introduces some level of overhead.
We have not investigated whether this overhead introduces any bias towards \Jajapy\ or \Cupaal.
While we do not expect it to do so, we also cannot say that we fully understand the overhead of Docker.
For completeness, this might have been better explored.

In \autoref{sec:exp_extra_scalability}, the values for the initial model values are not entirely random. Instead, they are designed to have repeated values, which was done to display the theoretical strengths of \Cupaal.
This method skews the model to favor \Cupaal, as the \gls{add} structure benefits from repeated values and, therefore, will display results that indicate \Cupaal\ as the stronger implementation.

This was done purely to research what was believed to be a strength of \Cupaal\ and to further the discussion on when \Cupaal\ is a good option to use over other tools, such as \Jajapy.
It would be interesting to explore the lmits of this initialization strategy for the \gls{bw} in general, as initialization of the \gls{sul} is a point of research on its own.


\subsection{Implementation Discussion}\label{subsec:implementation_discussion}
\Cupaal\ displays clear benefits when working with repeated values; however, it did not compare as favorably to \Jajapy\ as we expected.

Previous work indicated that \Cupaal\ overall was a stronger implementation, but with an entirely symbolic implementation, some potential bottlenecks have been observed.

Specifically in the update step of the \gls{bw} algorithm, as when working with \glspl{add} for just the $\alpha$ and $\beta$ steps, \Cupaal\ performed very well - much better than what is indicated for the complete \gls{bw} algorithm implemented here.

This suggests that there may be issues when updating the values when using \glspl{add}.
To further research this topic, a hybrid implementation could be provided.
This implementation would utilize \glspl{add} when calculating $\alpha$ and $\beta$ and then employ a recursive approach when updating values.

An implementation like this would require much conversion between matrices and \glspl{add}, but comparing a fully symbolic, a recursive, and a hybrid approach would give further insight into what \Cupaal\ struggles with.

For now, \Cupaal\ only measures the time taken to compute the \gls{bw} algorithm, but an interesting metric to compare would be the memory used.
If \Cupaal\ was discovered to require less memory than \Jajapy, even with more time needed for larger models, it could be a better choice in situations where memory was a constraint.
However, without a memory metric to compare, the decision can only be made based on the time required for computing \gls{bw}.

To allow \Cupaal\ and \Jajapy\ to work together, Pybind11 was used to create bindings between the two.
This decision was made without exploring other options. Therefore, it might not be the best-suited tool.
For now, bindings that worked were sufficient for the current iteration of \Cupaal, but further consideration should be given to whether a better tool exists.

The library used to manipulate \glspl{add} was \gls{cudd}, as it was what previous work had built upon.
A discussion at the time also raised the question of whether this is the best tool for the job, as other tools, such as Sylvan~\cite{van2017sylvan}, exist.
This discussion remains relevant and worth exploring.

\Cupaal\ is designed for the \gls{bw} algorithm, but it is worth exploring other algorithms that could benefit from a symbolic implementation.
An algorithm that could be explored could be the Viterbi algorithm.
By exploring other algorithms, the general benefits of using a symbolic approach can be better understood.


\subsection{Future Work}\label{subsec:future_work}
This section will discuss areas that might be worth exploring in future work.

\Cupaal\ only utilizes a single core. This is not an issue when comparing it to \Jajapy, as it can be limited to using only a single core.
However, improving \Cupaal\ to support multiple cores could be a worthwhile direction to explore, as it could provide a significant performance increase.

In an observation sequence, observations are grouped if they are identical, meaning that when computing these observations, we can factor in the number of identical observations and only compute one of them.

Expanding upon this idea involves utilizing prefixes and suffixes to enhance observations.
Many observations may not be entirely identical, but they could share a significant number of labels.

To leverage this, prefixes and suffixes of observations could be considered and grouped as is done currently.
Given that an observation sequence contains many observations that share prefixes and suffixes of labels, the gain could prove to be significant.

In the update step of \gls{bw} in \Cupaal, consideration is not made to check if the model worked on is a \gls{mc}.
This might be worth adding, as in these cases, unnecessary computation is made, as \glspl{mc} do not require the Emission function to be updated.

This is a minor consideration, as these values are ignored after they are computed, but it could be worth implementing.
