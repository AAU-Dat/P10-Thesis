\section{Methodology}\label{sec:methodology}
This section will provide an overview of different types of Decision Diagrams, how they each are structured, their differences and how they can be converted from one to another.

The different approaches that can be taken for the Baum-Welch algorithm will also be discussed, including the recursive, matrix-based, and \gls{add}-based approaches. The advantages and limitations of each approach will be highlighted.

Finally, the \gls{cudd} library will be introduced, which is a library for implementing and manipulating \glspl{bdd} and \glspl{add}.

\subsection{Decision Diagrams}\label{subsec:decision-diagrams}
\acrfullpl{bdd} are data structures for efficiently representing and manipulating Boolean functions.
They are a compressed representation of truth tables, capturing the logical structure of a function in a graph-based format by eliminating redundancy, reducing memory usage, and improving computational efficiency~\cite{bryant1986graph}.

A \gls{bdd} is a directed acyclic graph derived from a decision tree, where each non-terminal node represents a Boolean variable, edges correspond to binary assignments (0 or 1), and terminal nodes store function values (0 or 1).
To reduce the size of the decision tree, \glspl{bdd} exploit redundancy by merging equivalent substructures, resulting in a canonical form (when reduced and ordered) that allows for efficient operations such as function evaluation, equivalence checking, and Boolean operations~\cite{bryant1986graph}.

\glspl{bdd} have been widely used in formal verification, model checking, and logic synthesis due to their ability to compactly represent large Boolean functions while maintaining efficient computational properties.
However, in rare cases \glspl{bdd} can suffer from exponential blowup. This can occur particularly when dealing with functions that lack inherent structure or when representing numerical computations that go beyond Boolean logic.

\subsubsection{From BDDs to ADDs}\label{subsubsec:adds}
\acrfullpl{add} generalize the concept of \glspl{bdd} by allowing terminal nodes to take values beyond Boolean constants (0 and 1).
Instead of restricting values to true/false, \glspl{add} can store arbitrary numerical values, making them useful for representing and manipulating functions over discrete domains~\cite{bahar1997algebric}.
This generalization enables the efficient representation of functions such as cost functions~\cite{kwiatkowska2004probabilistic}, probabilities~\cite{baier1997symbolic}, and other numerical relationships that arise in probabilistic reasoning.

The fundamental structure of an \gls{add} remains similar to a \gls{bdd}, where a decision tree is compacted by merging redundant substructures.
However, instead of performing Boolean operations, \glspl{add} allow for arithmetic operations such as addition and multiplication, making them well-suited for applications like dynamic programming, \glspl{mdp}, and linear algebraic computations~\cite{bahar1997algebric}.

\subsection{Recursive vs. Matrix vs. ADD-based Approaches}\label{subsec:approaches}
When working with the Baum-Welch algorithm, different approaches can be taken to optimize computational efficiency.
Three common strategies are recursive, matrix-based, and \gls{add}-based approaches, each with distinct advantages and limitations.
\begin{itemize}
      \item \textbf{Recursive Approach:} Conceptually simple, recursion follows a divide-and-conquer strategy, and makes use of a dynamic programming approach. Previous calculations are used to build upon future calculations. These results are stored in a list or a map, so that they can be accessed when needed ~\cite[Chapter 4]{cormen2022introduction}.
      \item \textbf{Matrix Representation:} Reformulating algorithms using matrix operations leverages algebraic properties for parallel computation and efficient processing.
            By building upon the recursive approach, matrices provide an efficient method of accessing the stored results leading the faster computations overall~\cite[Chapter 4, 15 \& 28]{cormen2022introduction}.
      \item \textbf{ADD-based Approach:} \glspl{add} provide a compact representation that eliminates redundancy in recursive computations.
            By reusing previously computed substructures, they improve efficiency and reduce memory overhead~\cite{bahar1997algebric}.
            Compared to matrices, \glspl{add} can offer a more space-efficient alternative for structured data while extending \gls{bdd} techniques to handle both Boolean and numerical computations.
\end{itemize}

In this work we explore the benefits of \gls{add}-based approaches for solving complex problems, focusing on parameter estimation in \glspl{dtmc} and \glspl{ctmc}.
We compare the performance of \gls{add}-based algorithms against recursive-based implementations, highlighting the advantages of using \glspl{add} for efficient computation and memory management.

\subsection{CuDD}\label{subsec:cudd}
\acrfull{cudd} is a library for implementing and manipulating \glspl{bdd} and \glspl{add} developed at the University of Colorado.
The \gls{cudd} library~\cite{somenzi1997cudd} is a powerful tool for implementing and manipulating decision diagrams, including \glspl{bdd} and \glspl{add}.

Implemented in C, the \gls{cudd} library ensures high-performance execution and can be seamlessly integrated into C++ programs, which we utilize in this paper.
By leveraging the \gls{cudd} library, we demonstrate the benefits of \gls{add}-based approaches for solving parameter estimation problems in \glspl{dtmc} and \glspl{ctmc}.

In this project, we use the \gls{cudd} library to store \glspl{add} and perform operations on them.
Its optimized algorithms and efficient memory management enable symbolic handling of large and complex matrices, significantly improving performance compared to traditional methods.



\subsection{From Prism to CuPAAL}\label{subsec:from_prism_to_cupaal}
The models are encoded from Prism models to CuPAAL models. This is done by parsing the Prism model to Jajapy, using Stormpy.

The Jajapy model contains a matrix for it's transitions, a matrix for it's labels, and a vector for the initial state.
The model is passed to CuPAAL where these matrices and vectors are encoded into \glspl{add}, as a function $f \colon {0,1}^n \times {0,1}^n \to R$.

The Transition matrix is a $S\times S$ matrix, where $S=States$, and is encoded to an \gls{add}, by each row and column with a binary value. This value is determined based on the size of the matrix,
$n = \ceil{log_2(S)}$.

The label matrix is a $S\times L$ matrix, where $L=Labels$ and since there is no guarantee that $S = L$, the encoding is handled differently.
The matrix is instead treated as a list of vectors.
Each vector is encoded as square matrices, where each row or column (depending on the vector type) is duplicated, which is then encoded to a list of \glspl{add}.
As knowing the matrices are square and their dimensions help to simplify some of the symbolic operations.

The Initial state vector is encoded similarly to the label matrix, but only as a single \gls{add}.

We have not modified or extended the CUDD library.
All functionality used in our implementation is available through the standard CUDD library.
However, we adapted how we represent vectors to optimize our symbolic computations.

This is due to the structure of Decision Diagrams in CuPAAL, where keeping track of all the new binary values used for encoding from a matrix to an \gls{add} can add a layer of complexity for calculation.
Especially when computing operations that translate matrices to new dimensions, such as the Kronecker product.
This matrix-based approach enables efficient symbolic operations, as the Kronecker product can be calculated by taking the Hadamard product between a column matrix \gls{add} and a row matrix \gls{add}, simplifying what would otherwise be a more complex operation.


\subsection{Implementation to Jajapy}\label{subsec:implementation-to-jajapy}
This section will give an overview of how CuPAAL is implemented into Jajapy, using bindings between C++ and Python.

To implement CuPAAL into Jajapy, we create bindings between C++ and Python using the \texttt{pybind11} library~\cite{pybind11github}.
This allows us to call C++ functions from Python, enabling us to use CuPAAL in Jajapy.

We create a C++ bindings file, that defines the function we want to expose to Python, we call this function $bw\_wrapping\_function$.
This function takes model parameters from a Jajapy model as input, and transforms them to be used in CuPAAL.
We then calculate the Baum-Welch algorithm using CuPAAL, and return the results to Jajapy, as a tuple with the relevant values.
These being the initial distribution, the transitions and the emissions.

The C++ bindings file is then compiled to a shared library, which can be imported in Jajapy.
Jajapy can call the $bw\_wrapping\_function$ function, which will then call the CuPAAL implementation of the Baum-Welch algorithm.
In Jajapy, we add a nem boolean parameter for called symbolic to the fit function, which by default is false, and is set to true when the CuPAAL implementation is used.

When the parameter is true, the Jajapy model will call the $bw_wrapping_function$ function, which will then call the CuPAAL implementation of the Baum-Welch algorithm.


\subsection{Kronecker Product Implementation}\label{subsec:kronecker-product-implementation}
This section will provide an overview of how the Kronecker product is implemented in CuPAAL, using the row and column duplication method mentioned in \autoref{subsec:from_prism_to_cupaal}.

An example of this can be seen with the two vectors $\hat{A}$ and $\hat{B}$

Let $\hat{A} = \begin{bmatrix}
            1 \\
            2
      \end{bmatrix}$
and $\hat{B}=\begin{bmatrix}
            3 & 4
      \end{bmatrix}$.

The Kronecker product of these two vectors is computed as follows:
\begin{equation}
      \hat{A} \otimes \hat{B} = \begin{bmatrix}
            1 \cdot 3 & 1 \cdot 4 \\
            2 \cdot 3 & 2 \cdot 4
      \end{bmatrix} = \begin{bmatrix}
            3 & 4 \\
            6 & 8
      \end{bmatrix}.
      \label{eq:kronecker-product-example}
\end{equation}

Another way to calculate the Kronecker product is to expand the vectors into matrices.
$\hat{A}$ and $\hat{B}$ are expanded to be matrices, similar to how the matrix was treated as a list of vectors and then expanded to square matrices, as seen with the Label matrix.

Let $\mathbf{A} = \begin{bmatrix}
            1 & 1 \\
            2 & 2
      \end{bmatrix}$ and
$\mathbf{B} = \begin{bmatrix}
            3 & 4 \\
            3 & 4
      \end{bmatrix}$.

The Kronecker product of $\hat{A}$ and $\hat{B}$ can also be calculated, by using the Hadamard product of $\mathbf{A}$ and $\mathbf{B}$.
This is done as follows:

\begin{equation}
      \mathbf{A} \circ \mathbf{B} = \begin{bmatrix}
            1 \cdot 3 & 1 \cdot 4 \\
            2 \cdot 3 & 2 \cdot 4
      \end{bmatrix} = \begin{bmatrix}
            3 & 4 \\
            6 & 8
      \end{bmatrix}.
\end{equation}

Hereby showing that the Hadamard product can be used to compute the Kronecker product between two matrices, by using the row and column duplication method.

