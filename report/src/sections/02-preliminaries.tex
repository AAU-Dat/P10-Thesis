\section{Preliminaries}\label{sec:preliminaries}
This section provides an overview of the theoretical background necessary to understand the rest of the article.
We begin by defining the key concepts of a \gls{hmm} and a \gls{mc}, which are the two main models used in this report.


\subsection{Hidden Markov Model}\label{subsec:hmm}
\glspl{hmm} were introduced by Baum and Petrie in 1966~\cite{baum1966statistical} and have since been widely used in various fields, such as speech recognition~\cite{chavan2013overview}, bioinformatics~\cite{ciocchetta2009bio}, and finance~\cite{mamon2007hidden}.
\begin{definition}[Hidden Markov Model]
    A Hidden Markov Model (HMM) is a tuple $\mathcal{M} = (S, \mathcal{L}, \mathscr{l}, \tau,  \pi)$, where:
    \begin{itemize}
        \item $S$ is a finite set of states.
        \item $\mathcal{L}$ is a finite set of labels.
        \item $\mathscr{l}: S \rightarrow D(\mathcal{L})$ is the emission function.
        \item $\tau: S \rightarrow D(S)$ is the transition function.
        \item $\pi \in D(S)$ is the initial distribution.
    \end{itemize}
\end{definition}

$D(X)$ denotes the set of probability distributions over a  finite set $X$.
The emission function $\mathscr{l}$ describes the probability of emitting a label given a state.
The transition function $\tau$ describes the probability of transitioning from one state to another.
The initial distribution $\pi$ describes the probability of starting in a given state.
An \gls{hmm} is a statistical model that describes a system that evolves over time.
The system is assumed to hold the Markov property, meaning that the future state of the system only depends on the current state and not on the past states.
The system is also assumed to be unobservable, meaning that the states are hidden and cannot be directly observed.
Instead, the system emits observations, which are used to infer the hidden states.

An example of an \gls{hmm} is a weather model where the hidden state represents the actual weather (sunny, rainy, or cloudy), but we only observe indirect signals, such as whether someone is carrying an umbrella.

\subsection{Markov Chain}\label{subsec:mc}
A \gls{mc}, named after Andrei Markov, is a stochastic model widely used in different fields of study~\cite{Rabiner89}.
\begin{definition}[Markov Chain]
    A Markov Chain (MC) is a tuple $\mathcal{M} = (S, \mathcal{L}, \mathscr{l}, \tau,  \pi)$ identical to the HMM structure above except that the emission function is
    \emph{deterministic}: for every $s\in S$ there is a single label
    $l=\mathscr{l}(s)$ emitted with probability 1.
\end{definition}
In other words, the emission function $\mathscr{l}$ is a one-to-one mapping from states to labels.

A common example of an MC is a board game where a player moves between squares based on dice rolls.
Each square corresponds to a state, the dice rolls determine the transition probabilities, and the current square (state) is directly observable.

\subsection{Conversion between MCs and HMMs}\label{subsec:mc_hmm_conversion}
In this section, we will discuss the conversion between \glspl{mc} and \glspl{hmm}.
This conversion is important because it allows us to use the same algorithms and techniques for both models, even though they have different properties.
From the definition of a \gls{mc}, we can see that it is a special case of an \gls{hmm} where the emission function is deterministic.

\subsubsection{Markov Chains to Hidden Markov Models}\label{subsec:mc2hmm}
This conversion is straightforward because the components of the \gls{mc} and \gls{hmm} are the same.
\begin{definition}[Markov Chain to Hidden Markov Model]
    Given a \gls{hmm} $\mathcal{M} = (S, \mathcal{L}, \mathscr{l}, \tau,  \pi)$, we can convert it into a \gls{mc} $\mathcal{M}' = (S', \mathcal{L}', \mathscr{l}', \tau',  \pi')$ by defining the components as follows:
    \begin{itemize}
        \item $S' = S$.
        \item $\mathcal{L}' = \mathcal{L}$.
        \item $\mathscr{l}' =  \begin{cases}
                      1 & l=\mathscr{l}(s) \\
                      0 & \text{otherwise}
                  \end{cases}$
        \item $\tau' = \tau$.
        \item $\pi' = \pi$.
    \end{itemize}
\end{definition}
The \gls{mc} $\mathcal{M}'$ is equivalent to the \gls{hmm} $\mathcal{M}$, meaning that they have the same transition and emission probabilities.
The only difference is that the \gls{mc} has a trivial emission function, meaning that each state emits a single label with probability 1.

\subsubsection{Hidden Markov Models to Markov Chains}\label{subsec:hmm2mc}
Converting a \gls{hmm} to an equivalent \gls{mc} is more complex.
In a \gls{hmm}, the observations are probabilistically related to the states, which introduces ambiguity, as multiple states can emit the same observation.
To create a fully observable \gls{mc} that captures the behavior of a \gls{hmm}, we must encode both the hidden state and the emitted label into the state space.
\begin{definition}[Hidden Markov Model to Markov Chain]
    Conversely, let
    \(
    \mathcal{M}
    = (S, \mathcal{L}, \mathscr{l}, \tau, \pi)
    \)
    be a \gls{hmm}.
    We define the observable \gls{mc}
    \(
    \mathcal{M}'
    = (S', \mathcal{L}', \mathscr{l}', \tau', \pi')
    \)
    by:
    \begin{itemize}
        \item $S' = \{(s,l)\in S\times\mathcal{L}\}$%, the new state space encodes both hidden states and their possible emitted labels,
        \item $\mathcal{L}' = \mathcal{L}$%, the label space remains unchanged,
        \item $\mathscr{l}'(s,l) = l$%, each new state deterministically emits its label,
        \item $\tau'\big((s,l),(s',l')\big) = \tau(s,s')\;\mathscr{l}(s')(l')$%, the transition function is modified to account for the emission probabilities,
        \item $\pi'(s,l) = \pi(s)\;\mathscr{l}(s)(l)$%, the initial distribution is modified to account for the emission probabilities.
    \end{itemize}
    \textbf{Remark.}
    Here each ' symbol refers to an object of the \emph{derived MC}.  In
    particular, $\tau'$ is \emph{not} the original HMM transition; it acts on the
    expanded state space $S'$ and already incorporates the emission probability for
    the label $l'$.

    The mapping increases the state space size from $|S|$ to at most
    $|S|\cdot|\mathcal{L}|$, but yields a fully observable system amenable to
    standard MC analysis.
\end{definition}

The conversion between \glspl{mc} and \glspl{hmm} is important because it allows us to use the same algorithms and techniques for both models, even though they have different properties.
