\section{Implementation}\label{sec:implementation}
This section will provide an overview of the implementation of \Cupaal.
This will include the Baum-Welch algorithm, and how \Cupaal~is integrated into \Jajapy, creating \JajapyTwo.

\subsection{Motivation for CuPAAL}\label{subsec:motivation-for-cupaal}
The motivation for \Cupaal is to provide a more efficient and scalable implementation of the Baum-Welch algorithm for parameter estimation.
Specifically, we aim to improve the performance of the algorithm when dealing with large and complex models, and improve upon the existing limitations of the Baum-Welch algorithm in \Jajapy.

\subsubsection{Recursive vs. Matrix vs. ADD-based Approaches}\label{subsec:approaches}
When working with the Baum-Welch algorithm, different approaches can be taken to optimize computational efficiency.
Three common strategies are recursive, matrix-based, and \gls{add}-based approaches, each with distinct advantages and limitations.

\begin{itemize}
    \item \textbf{Recursive Approach:} Conceptually simple, recursion follows a divide-and-conquer strategy, and makes use of a dynamic programming approach. Previous calculations are used to build upon future calculations. These results are stored in a list or a map, so that they can be accessed when needed ~\cite[Chapter 4]{cormen2022introduction}.
    \item \textbf{Matrix Representation:} Reformulating algorithms using matrix operations leverages algebraic properties for parallel computation and efficient processing.
          By building upon the recursive approach, matrices provide an efficient method of accessing the stored results leading the faster computations overall~\cite[Chapter 4, 15 \& 28]{cormen2022introduction}.
    \item \textbf{ADD-based Approach:} \glspl{add} provide a compact representation that eliminates redundancy in recursive computations.
          By reusing previously computed substructures, they improve efficiency and reduce memory overhead~\cite{bahar1997algebric}.
          Compared to matrices, \glspl{add} can offer a more space-efficient alternative for structured data while extending \gls{bdd} techniques to handle both Boolean and numerical computations.
\end{itemize}

In this work we explore the benefits of \gls{add}-based approaches for solving complex problems, focusing on parameter estimation in \glspl{dtmc} and \glspl{ctmc}.
We compare the performance of \gls{add}-based algorithms against recursive-based implementations, highlighting the advantages of using \glspl{add} for efficient computation and memory management.

\subsection{Evolution of \Cupaal}\label{subsec:evolution_of_cupaal}
\Cupaal is a C++ library that implements the Baum-Welch algorithm for parameter estimation, and has evolved over time.

The initial version of \Cupaal was called \textit{SUDD}, which was a partial implementation of the Baum-Welch algorithm, using \glspl{add}.
This version was mainly focused on displaying the efficiency of \glspl{add} for parameter estimation problems, and was not fully functional.
The next version of \Cupaal was called \textit{CuPAAL}, which was a complete implementation of the Baum-Welch algorithm, using \glspl{add}, however this only supported \glspl{hmm}, and was only designed to make use of a single observation.



\subsubsection{CuDD}\label{subsec:cudd}
\acrfull{cudd} is a library for implementing and manipulating \glspl{bdd} and \glspl{add} developed at the University of Colorado.
The \gls{cudd} library~\cite{somenzi1997cudd} is a powerful library for implementing and manipulating various types of decision diagrams, including \glspl{bdd} and \glspl{add}.

Implemented in C, the \gls{cudd} library ensures high-performance execution and can be seamlessly integrated into C++ programs, which we utilize in \Cupaal.
By leveraging the \gls{cudd} library, we demonstrate the benefits of \gls{add}-based approaches for solving parameter estimation problems in \glspl{dtmc} and \glspl{ctmc}.

In this project, we use the \gls{cudd} library to store \glspl{add} and perform operations on them.
Its optimized algorithms and efficient memory management enable symbolic handling of large and complex matrices, significantly improving performance compared to traditional methods.

\subsubsection{From Prism to CuPAAL}\label{subsec:from_prism_to_cupaal}
The models are encoded from Prism models to CuPAAL models. This is done by parsing the Prism model to Jajapy, using Stormpy.

The Jajapy model contains a matrix for it's transitions, a matrix for it's labels, and a vector for the initial state.
The model is passed to CuPAAL where these matrices and vectors are encoded into \glspl{add}, as a function $f \colon {0,1}^n \times {0,1}^n \to R$.

The Transition matrix is a $S\times S$ matrix, where $S=States$, and is encoded to an \gls{add}, by each row and column with a binary value. This value is determined based on the size of the matrix,
$n = \ceil{log_2(S)}$.

The label matrix is a $S\times L$ matrix, where $L=Labels$ and since there is no guarantee that $S = L$, the encoding is handled differently.
The matrix is instead treated as a list of vectors.
Each vector is encoded as square matrices, where each row or column (depending on the vector type) is duplicated, which is then encoded to a list of \glspl{add}.
Knowing the exact dimensions of matrices and that they are square helps to simplify some of the symbolic operations.
An example of this will be displayed in \autoref{subsec:kronecker-product-implementation}.

The Initial state vector is encoded similarly to the label matrix, but only as a single \gls{add}.

We have not modified or extended the CUDD library.
All functionality used in our implementation is available through the standard CUDD library.

\subsubsection{Kronecker Product Implementation}\label{subsec:kronecker-product-implementation}
This section will provide an overview of how the Kronecker product is implemented in CuPAAL, using the row and column duplication method mentioned in \autoref{subsec:from_prism_to_cupaal}.

The structure of Decision Diagrams in CuPAAL, where keeping track of all the new binary values used for encoding from a matrix to an \gls{add} can add a layer of complexity for calculation.
Especially when computing operations that translate matrices to new dimensions, such as the Kronecker product.
This matrix-based approach enables efficient symbolic operations, as the Kronecker product can be calculated by taking the Hadamard product between a column matrix \gls{add} and a row matrix \gls{add}, simplifying what would otherwise be a more complex operation.

An example of this can be seen with the two vectors $\hat{A}$ and $\hat{B}$

Let $\hat{A} = \begin{bmatrix}
        1 \\
        2
    \end{bmatrix}$
and $\hat{B}=\begin{bmatrix}
        3 & 4
    \end{bmatrix}$.

The Kronecker product of these two vectors is computed as follows:
\begin{equation}
    \hat{A} \otimes \hat{B} = \begin{bmatrix}
        1 \cdot 3 & 1 \cdot 4 \\
        2 \cdot 3 & 2 \cdot 4
    \end{bmatrix} = \begin{bmatrix}
        3 & 4 \\
        6 & 8
    \end{bmatrix}.
    \label{eq:kronecker-product-example}
\end{equation}

Another way to calculate the Kronecker product is to expand the vectors into matrices.
$\hat{A}$ and $\hat{B}$ are expanded to be matrices, similar to how the matrix was treated as a list of vectors and then expanded to square matrices, as seen with the Label matrix.

Let $\mathbf{A} = \begin{bmatrix}
        1 & 1 \\
        2 & 2
    \end{bmatrix}$ and
$\mathbf{B} = \begin{bmatrix}
        3 & 4 \\
        3 & 4
    \end{bmatrix}$.

The Kronecker product of $\hat{A}$ and $\hat{B}$ can also be calculated, by using the Hadamard product of $\mathbf{A}$ and $\mathbf{B}$.
This is done as follows:


\begin{equation}
    \mathbf{A} \circ \mathbf{B} = \begin{bmatrix}
        1 \cdot 3 & 1 \cdot 4 \\
        2 \cdot 3 & 2 \cdot 4
    \end{bmatrix} = \begin{bmatrix}
        3 & 4 \\
        6 & 8
    \end{bmatrix}.
\end{equation}


Hereby showing that the Hadamard product can be used to compute the Kronecker product between two matrices, by using the row and column duplication method.

\subsection{Implementation to Jajapy}\label{subsec:implementation-to-jajapy}
This section will give an overview of how \Cupaal\ is implemented into \Jajapy, using bindings between C++ and Python.
\autoref{fig:cupaal-jajapy-architecture} shows the overall architecture of the implementation.


\begin{figure}[htb!]
    \centering
    \input{figures/cupaal-jajapy-architecture.tex}
    \caption{Architecture of \Cupaal\ combined with \Jajapy.}
    \label{fig:cupaal-jajapy-architecture}
\end{figure}


\Cupaal\ consists of two main componenets, the main function and the \gls{bw} library.
Both of these are compiled to an executable program, called \texttt{cupaal.exe}, which can be used to run the Baum-Welch algorithm on a given model.

To implement \Cupaal\ into \Jajapy, we create bindings between C++ and Python using the \texttt{pybind11} library~\cite{pybind11github}.
This allows us to call C++ functions from Python, enabling us to use \Cupaal\ in \Jajapy.

We create a C++ bindings file, that uses the \gls{bw} library from \Cupaal\ and defines the function we want to expose to Python, we call this function $cupaal\_bw\_symbolic$, seen in~\autoref{lst:cupaal-bindings}.
This function takes model parameters from a \Jajapy\ model as input, and transforms them to be used in \Cupaal.


\begin{listing}[htb!]
    \begin{minted}{cpp}
// Some parameters have been removed for brevity
cupaal_markov_model cupaal_bw_symbolic(vector<string>& states, vector<string>& labels, vector<vector<string>>& observations, vector<double>& initial_distribution, vector<double>& transitions, vector<double>& emissions, int max_iterations = 100, double epsilon = 1e-2){
    MarkovModel model(states, labels, initial_distribution, transitions, emissions, observations);
    cupaal_markov_model model_data;
    chrono::seconds time = chrono::seconds(3600);
    model.baum_welch_multiple_observations(
        max_iterations, epsilon, time);

    // Removed output and result path for brevity
    model_data.initial_distribution = model.initial_distribution;
    model_data.transitions = model.transitions;
    model_data.emissions = model.emissions;
    
    Cudd_Quit(model.manager);
    return model_data;
}
      \end{minted}
    \caption{C++ bindings file for CuPAAL}
    \label{lst:cupaal-bindings}
\end{listing}


\autoref{lst:cupaal-bindings} calculates the Baum-Welch algorithm using~\autoref{lst:baum-welch-multiple-observations} from \Cupaal, and returns the results to Jajapy, as a tuple with the relevant values.
These being the initial distribution, the transitions and the emissions.


\begin{listing}[htb!]
    \begin{minted}{cpp}
        void baum_welch_multiple_observations(unsigned int max_iterations = 100, double epsilon = 1e-6, chrono::seconds time = chrono::seconds(3600));
        \end{minted}
    \caption{Prototype of the function used to run the Baum-Welch algorithm on multiple observations in CuPAAL.}
    \label{lst:baum-welch-multiple-observations}
\end{listing}


The C++ bindings file is then compiled to a shared library, which can be imported in Jajapy.
Jajapy can call the $cupaal\_bw\_symbolic$ function, which will then call the CuPAAL implementation of the Baum-Welch algorithm.

We create a new function in Jajapy, called \texttt{\_bw\_symbolic}, which is used to call the \Cupaal\ implementation of the Baum-Welch algorithm, as seen in~\autoref{lst:jajapy-bw-symbolic}.


\begin{listing*}[htb!]
    \begin{minted}{python}
def _bw_symbolic(self, max_iteration = 100, epsilon = 1e-2, outputPath = "", resultPath = ""):
    try:
        import libcupaal_bindings
    except ModuleNotFoundError:
        print("Cannot find module")

    states = [str(i) for i in range (self.h.nb_states)]
    labels = list(set(self.h.labelling))
    observations = []
    for times, sequences in zip(self.training_set.times, self.training_set.sequences):
        for i in range(times):
            observations.append(list(sequences))
    initial_state = self.h.initial_state.tolist()
    transitions = self.h.matrix.flatten().tolist()
    emissions = zeros((len(labels), self.h.nb_states))
    for row in range(len(labels)):
        for col in range(self.h.nb_states):
            if self.h.labelling[col] == labels[row]:
                emissions[row][col] = 1
    emissions = emissions.flatten().tolist()

    cupaal_model = libcupaal_bindings.cupaal_bw_symbolic( states, labels, observations, initial_state, transitions, emissions, max_iteration, epsilon, outputPath, resultPath)
    self.h.initial_state = array(cupaal_model.initial_distribution)
    self.h.matrix = array(cupaal_model.transitions).reshape( self.h.nb_states, self.h.nb_states)
    self.h.emissions = array(cupaal_model.emissions).reshape( len(labels), self.h.nb_states)
    return self.h
      \end{minted}
    \caption{Jajapy's implementation of the Baum-Welch algorithm using CuPAAL.}
    \label{lst:jajapy-bw-symbolic}
\end{listing*}


The fit function in Jajapy is modified to call the \texttt{\_bw\_symbolic} function when a new parameter called \texttt{symbolic} is set to true, as seen in~\autoref{lst:jajapy-fit-cupaal}.


\begin{listing}[htb!]
    \begin{minted}{python}
# Some parameters have been removed for brevity
def fit(self, output_file: str, output_file_prism: str, epsilon: float, max_it: int, symbolic: bool):
    # Removed preparation and settings number of processes, for brevity
    if symbolic :
        return self._bw_symbolic(max_it, epsilon, output_file, output_file_prism)
    else:
        return self._bw(max_it, pp, epsilon, output_file, output_file_prism, verbose, stormpy_output, return_data)
      \end{minted}
    \caption{Jajapy's fit function, which calls the CuPAAL implementation of the Baum-Welch algorithm when symbolic is set to true.}
    \label{lst:jajapy-fit-cupaal}
\end{listing}


When the parameter is true, the Jajapy model will call the~\autoref{lst:jajapy-bw-symbolic} function, which will then call the CuPAAL implementation of the Baum-Welch algorithm.

\subsection{Integration Discussion}\label{subsec:integration-discussion}
The integration of \Cupaal\ into \Jajapy\ has been successful, allowing us to leverage the Baum-Welch algorithm for parameter estimation in \glspl{dtmc} and \glspl{ctmc}.



% Implementation
% Motivation for CuPAAL
% recursive vs. matrix vs. ADD-based approaches
% CuPAAL and how it evolved and what it is now
%Sudd for HMMs
% CuDD and how it is used in CuPAAL
% Kronecker product implementation - Remember code and math
% Implementation of CuPAAL in Jajapy
% Bindings
%Discussion of the implementation