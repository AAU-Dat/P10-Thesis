\section{Results}\label{sec:results}
In this section, we present the results of our experiments, which are divided into two main parts: the first part focuses on the scalability of \Jajapy\ and \Cupaal\ in terms of time and scalability, while the second part evaluates the accuracy of both tools.

The experiments were conducted on a machine with the specifications and environment listed in \autoref{sec:machine_specs}.


\subsection{Scalability}\label{subsec:scalability}
These results are the time taken to train a model, based on two parameters: the number of states, and the length of the observations in the model increasing.


\begin{table}[htb!]
    \centering
    \caption{Leader Sync model variations training times in seconds.}
    \label{tab:leader_results}
    \begin{tabular}{lrrrr}
        \toprule
        model & states & length & jajapy (s) & cupaal (s) \\
        \midrule
        3.2   & 26     & 25     & 1.38       & 0.26       \\
        3.2   & 26     & 50     & 1.95       & 0.14       \\
        3.2   & 26     & 100    & 4.09       & 0.23       \\
        3.3   & 69     & 25     & 7.95       & 2.46       \\
        3.3   & 69     & 50     & 11.20      & 1.59       \\
        3.3   & 69     & 100    & 19.65      & 1.75       \\
        3.4   & 147    & 25     & 27.10      & 8.54       \\
        3.4   & 147    & 50     & 42.57      & 9.20       \\
        3.4   & 147    & 100    & 84.02      & 9.90       \\
        4.2   & 61     & 25     & 15.68      & 11.18      \\
        4.2   & 61     & 50     & 24.87      & 13.56      \\
        4.2   & 61     & 100    & 52.11      & 11.24      \\
        4.3   & 274    & 25     & 194.88     & 231.28     \\
        4.3   & 274    & 50     & 414.30     & 379.21     \\
        4.3   & 274    & 100    & 447.83     & 117.78     \\
        4.4   & 812    & 25     & 1846.68    & 3324.83    \\
        4.4   & 812    & 50     & 2290.28    & 1848.44    \\
        4.4   & 812    & 100    & 5652.14    & 3447.56    \\
        5.2   & 141    & 25     & 95.59      & 104.71     \\
        5.2   & 141    & 50     & 342.05     & 553.66     \\
        5.2   & 141    & 100    & 798.73     & 982.97     \\
        5.3   & 1050   & 25     & 4586.86    & 10906.91   \\
        5.3   & 1050   & 50     & 7791.95    & 10405.75   \\
        5.3   & 1050   & 100    & 9821.74    & 5992.51    \\
        \bottomrule
    \end{tabular}
\end{table}


The results for the leader sync model are displayed in \autoref{tab:leader_results} and \autoref{fig:leader_results}, and show the time it takes to train a model, given the number of states and observation length.
Only the training time is considered; the initialization of the programs is not a factor in these numbers.

Contrary to our expectations, the data does not show a clear difference in the time taken to train the leader sync model between \Jajapy\ and \Cupaal\ for \glspl{dtmc}.

For very small models, the running time does not matter too much, but we observe an initial overhead related to \Jajapy. This is likely related to the general consensus that Python is a slower language than C in general.

in general, more states mean longer running time, but interestingly, variations with similar number of states may have very different training times. The most obvious example is the 3.4 and 5.2 models, with 147 and 141 states respectively, but the 5.2 is much slower, especially in \Cupaal.

Initially, we only had data for observations of length 25, and the data under those conditions suggested that \Jajapy scaled quite a bit better than \Cupaal.

To explore this behaviour, we extended the experiment to contain data for observations of different lengths, and now our observations are more in line with our expectations.
\Jajapy gets slower at a pace roughly linear with the length of the observations; doubling the observation length doubles the run time of \Jajapy.
This is not the case for \Cupaal.



% This is likely due to the smaller size of the model, not having as much redundant calculations to avoid, which is the main advantage of the symbolic approach used in CuPAAL.


From \autoref{fig:leader_results}

\begin{figure*}[htb!]
    \centering
    \input{figures/results/3d-scalability.pgf}
    \caption{Plot of the run time of \Jajapy\ and \Cupaal\ for the leader sync models, given the number of states and the length of the observations.}
    \label{fig:leader_results}
\end{figure*}


\subsection{Accuracy}\label{subsec:accuracy}
